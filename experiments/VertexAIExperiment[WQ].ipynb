{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"VertexAIPlayground.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"copyright","executionInfo":{"status":"ok","timestamp":1638509421667,"user_tz":480,"elapsed":6,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["# Copyright 2021 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"title"},"source":["# Vertex SDK: Custom training image classification model for online prediction with explainabilty\n","\n","<table align=\"left\">\n","  <td>\n","    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/sdk_custom_image_classification_online_explain.ipynb\">\n","      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n","    </a>\n","  </td>\n","  <td>\n","    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/sdk_custom_image_classification_online_explain.ipynb\">\n","      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n","      View on GitHub\n","    </a>\n","  </td>\n","  <td>\n","    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/sdk_custom_image_classification_online_explain.ipynb\">\n","      Open in Google Cloud Notebooks\n","    </a>\n","  </td>\n","</table>\n","<br/><br/><br/>"]},{"cell_type":"markdown","metadata":{"id":"overview:custom,xai"},"source":["## Overview\n","\n","\n","This tutorial demonstrates how to use the Vertex SDK to train and deploy a custom image classification model for online prediction with explanation."]},{"cell_type":"markdown","metadata":{"id":"dataset:custom,cifar10,icn"},"source":["### Dataset\n","\n","The dataset used for this tutorial is the [CIFAR10 dataset](https://www.tensorflow.org/datasets/catalog/cifar10) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). The version of the dataset you will use is built into TensorFlow. The trained model predicts which type of class an image is from ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck."]},{"cell_type":"markdown","metadata":{"id":"objective:custom,training,online_prediction,xai"},"source":["### Objective\n","\n","In this tutorial, you create a custom model from a Python script in a Google prebuilt Docker container using the Vertex SDK, and then do a prediction with explanations on the deployed model by sending data. You can alternatively create custom models using `gcloud` command-line tool or online using Cloud Console.\n","\n","The steps performed include:\n","\n","- Create a Vertex custom job for training a model.\n","- Train a TensorFlow model.\n","- Retrieve and load the model artifacts.\n","- View the model evaluation.\n","- Set explanation parameters.\n","- Upload the model as a Vertex `Model` resource.\n","- Deploy the `Model` resource to a serving `Endpoint` resource.\n","- Make a prediction with explanation.\n","- Undeploy the `Model` resource."]},{"cell_type":"markdown","metadata":{"id":"costs"},"source":["### Costs\n","\n","This tutorial uses billable components of Google Cloud:\n","\n","* Vertex AI\n","* Cloud Storage\n","\n","Learn about [Vertex AI\n","pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n","pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n","Calculator](https://cloud.google.com/products/calculator/)\n","to generate a cost estimate based on your projected usage."]},{"cell_type":"markdown","metadata":{"id":"setup_local"},"source":["### Set up your local development environment\n","\n","If you are using Colab or Google Cloud Notebook, your environment already meets all the requirements to run this notebook. You can skip this step.\n","\n","Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n","\n","- The Cloud Storage SDK\n","- Git\n","- Python 3\n","- virtualenv\n","- Jupyter notebook running in a virtual environment with Python 3\n","\n","The Cloud Storage guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n","\n","1. [Install and initialize the SDK](https://cloud.google.com/sdk/docs/).\n","\n","2. [Install Python 3](https://cloud.google.com/python/setup#installing_python).\n","\n","3. [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3.\n","\n","4. Activate that environment and run `pip3 install Jupyter` in a terminal shell to install Jupyter.\n","\n","5. Run `jupyter notebook` on the command line in a terminal shell to launch Jupyter.\n","\n","6. Open this notebook in the Jupyter Notebook Dashboard.\n"]},{"cell_type":"markdown","metadata":{"id":"install_aip:mbsdk"},"source":["## Installation\n","\n","Install the latest version of Vertex SDK for Python."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YesqSL5a-VsE","executionInfo":{"status":"ok","timestamp":1638509437771,"user_tz":480,"elapsed":16109,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"53155fa3-0c72-497d-c53d-4082268aed7f"},"source":["import os\n","\n","# Google Cloud Notebook\n","if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n","    USER_FLAG = \"--user\"\n","else:\n","    USER_FLAG = \"\"\n","\n","! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting google-cloud-aiplatform\n","  Downloading google_cloud_aiplatform-1.7.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 143 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 163 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 215 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 225 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 235 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 256 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 286 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 307 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 317 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 327 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 337 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 358 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 368 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 378 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 389 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 399 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 409 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 419 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 430 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 440 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 450 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 460 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 471 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 481 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 501 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 512 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 522 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 532 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 552 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 563 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 573 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 583 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 593 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 604 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 614 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 624 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 634 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 645 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 655 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 665 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 675 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 686 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 696 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 706 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 716 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 727 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 737 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 747 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 757 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 768 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 778 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 788 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 798 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 808 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 819 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 829 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 839 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 849 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 860 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 870 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 880 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 890 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 901 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 911 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 921 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 931 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 942 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 952 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 962 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 972 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 983 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 993 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.0 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.0 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.0 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.6 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 9.1 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.21.0)\n","Collecting proto-plus>=1.10.1\n","  Downloading proto_plus-1.19.8-py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 4.3 MB/s \n","\u001b[?25hCollecting google-cloud-storage<2.0.0dev,>=1.32.0\n","  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 98.0 MB/s \n","\u001b[?25hRequirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.26.3)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (21.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.15.0)\n","Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.35.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (57.4.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2018.9)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.23.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.17.3)\n","Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.42.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.8)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.0.3)\n","Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (0.4.1)\n","Collecting google-cloud-core<2.0dev,>=1.0.3\n","  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n","Collecting google-cloud-storage<2.0.0dev,>=1.32.0\n","  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 93.2 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 93.5 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.42.1-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 100.0 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 75.0 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 82.3 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 94.3 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 82.0 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 97.0 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 96.1 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 95.8 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n","\u001b[K     |████████████████████████████████| 103 kB 83.3 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 8.6 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 8.3 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 8.7 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n","\u001b[K     |████████████████████████████████| 96 kB 7.3 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n","\u001b[K     |████████████████████████████████| 96 kB 7.2 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n","\u001b[K     |████████████████████████████████| 96 kB 7.0 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n","\u001b[K     |████████████████████████████████| 92 kB 13.8 MB/s \n","\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n","\u001b[K     |████████████████████████████████| 92 kB 13.6 MB/s \n","\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n","Collecting google-cloud-core<2.0dev,>=1.0.3\n","  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n","  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n","  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n","  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n","  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n","  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n","INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n","  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n","  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n","  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n","  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n","  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n","  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n","  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n","INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n","Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n","  Downloading google_cloud_bigquery-2.31.0-py2.py3-none-any.whl (205 kB)\n","\u001b[K     |████████████████████████████████| 205 kB 80.2 MB/s \n","\u001b[?25hCollecting google-resumable-media<3.0dev,>=0.6.0\n","  Downloading google_resumable_media-2.1.0-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n","Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n","  Downloading google_cloud_bigquery-2.30.1-py2.py3-none-any.whl (203 kB)\n","\u001b[K     |████████████████████████████████| 203 kB 93.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.30.0-py2.py3-none-any.whl (203 kB)\n","\u001b[K     |████████████████████████████████| 203 kB 100.9 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.29.0-py2.py3-none-any.whl (203 kB)\n","\u001b[K     |████████████████████████████████| 203 kB 104.4 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.28.1-py2.py3-none-any.whl (202 kB)\n","\u001b[K     |████████████████████████████████| 202 kB 80.3 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.28.0-py2.py3-none-any.whl (202 kB)\n","\u001b[K     |████████████████████████████████| 202 kB 95.0 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.27.1-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 101.4 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.27.0-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 101.5 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.26.0-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 84.1 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.25.2-py2.py3-none-any.whl (200 kB)\n","\u001b[K     |████████████████████████████████| 200 kB 70.2 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.25.1-py2.py3-none-any.whl (200 kB)\n","\u001b[K     |████████████████████████████████| 200 kB 78.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.25.0-py2.py3-none-any.whl (200 kB)\n","\u001b[K     |████████████████████████████████| 200 kB 96.2 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.24.1-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 99.9 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.24.0-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 104.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.23.3-py2.py3-none-any.whl (196 kB)\n","\u001b[K     |████████████████████████████████| 196 kB 101.2 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.23.2-py2.py3-none-any.whl (196 kB)\n","\u001b[K     |████████████████████████████████| 196 kB 102.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.23.1-py2.py3-none-any.whl (196 kB)\n","\u001b[K     |████████████████████████████████| 196 kB 100.9 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.23.0-py2.py3-none-any.whl (196 kB)\n","\u001b[K     |████████████████████████████████| 196 kB 102.2 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.22.1-py2.py3-none-any.whl (195 kB)\n","\u001b[K     |████████████████████████████████| 195 kB 95.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.22.0-py2.py3-none-any.whl (194 kB)\n","\u001b[K     |████████████████████████████████| 194 kB 102.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.21.0-py2.py3-none-any.whl (193 kB)\n","\u001b[K     |████████████████████████████████| 193 kB 94.1 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n","\u001b[K     |████████████████████████████████| 189 kB 102.0 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.19.0-py2.py3-none-any.whl (188 kB)\n","\u001b[K     |████████████████████████████████| 188 kB 102.7 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.18.0-py2.py3-none-any.whl (188 kB)\n","\u001b[K     |████████████████████████████████| 188 kB 105.8 MB/s \n","\u001b[?25hCollecting google-resumable-media<2.0dev,>=0.6.0\n","  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 6.2 MB/s \n","\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n","  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.6)\n","Collecting protobuf>=3.12.0\n","  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 58.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.0.4)\n","Installing collected packages: protobuf, google-crc32c, proto-plus, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-bigquery, google-cloud-aiplatform\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: google-resumable-media\n","    Found existing installation: google-resumable-media 0.4.1\n","    Uninstalling google-resumable-media-0.4.1:\n","      Successfully uninstalled google-resumable-media-0.4.1\n","  Attempting uninstall: google-cloud-core\n","    Found existing installation: google-cloud-core 1.0.3\n","    Uninstalling google-cloud-core-1.0.3:\n","      Successfully uninstalled google-cloud-core-1.0.3\n","  Attempting uninstall: google-cloud-storage\n","    Found existing installation: google-cloud-storage 1.18.1\n","    Uninstalling google-cloud-storage-1.18.1:\n","      Successfully uninstalled google-cloud-storage-1.18.1\n","  Attempting uninstall: google-cloud-bigquery\n","    Found existing installation: google-cloud-bigquery 1.21.0\n","    Uninstalling google-cloud-bigquery-1.21.0:\n","      Successfully uninstalled google-cloud-bigquery-1.21.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.18.0 which is incompatible.\u001b[0m\n","Successfully installed google-cloud-aiplatform-1.7.1 google-cloud-bigquery-2.18.0 google-cloud-core-1.7.2 google-cloud-storage-1.41.1 google-crc32c-1.3.0 google-resumable-media-1.3.3 proto-plus-1.19.8 protobuf-3.19.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"install_storage"},"source":["Install the latest GA version of *google-cloud-storage* library as well."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":784},"id":"4emKpGx6-VsF","executionInfo":{"status":"ok","timestamp":1638509442289,"user_tz":480,"elapsed":4537,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"00b7c556-5d8a-44a1-be8a-4885919ff6b5"},"source":["! pip3 install -U google-cloud-storage $USER_FLAG"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (1.41.1)\n","Collecting google-cloud-storage\n","  Using cached google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.3.3)\n","Collecting google-api-core<3.0dev,>=1.29.0\n","  Downloading google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\n","\u001b[K     |████████████████████████████████| 95 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.7.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (3.19.1)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (2.23.0)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.35.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.15.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (57.4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (1.53.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.4)\n","  Downloading google_api_core-1.31.4-py2.py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (21.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2018.9)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (3.0.6)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.10.8)\n","Installing collected packages: google-api-core, google-cloud-storage\n","  Attempting uninstall: google-api-core\n","    Found existing installation: google-api-core 1.26.3\n","    Uninstalling google-api-core-1.26.3:\n","      Successfully uninstalled google-api-core-1.26.3\n","  Attempting uninstall: google-cloud-storage\n","    Found existing installation: google-cloud-storage 1.41.1\n","    Uninstalling google-cloud-storage-1.41.1:\n","      Successfully uninstalled google-cloud-storage-1.41.1\n","Successfully installed google-api-core-1.31.4 google-cloud-storage-1.43.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"install_tensorflow","executionInfo":{"status":"ok","timestamp":1638509442290,"user_tz":480,"elapsed":6,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["if os.getenv(\"IS_TESTING\"):\n","    ! pip3 install --upgrade tensorflow $USER_FLAG"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"install_cv2","executionInfo":{"status":"ok","timestamp":1638509443452,"user_tz":480,"elapsed":4,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["if os.getenv(\"IS_TESTING\"):\n","    ! apt-get update && apt-get install -y python3-opencv-headless\n","    ! apt-get install -y libgl1-mesa-dev\n","    ! pip3 install --upgrade opencv-python-headless $USER_FLAG"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"restart"},"source":["### Restart the kernel\n","\n","Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."]},{"cell_type":"code","metadata":{"id":"z1AYThI_-VsG","executionInfo":{"status":"ok","timestamp":1638509445832,"user_tz":480,"elapsed":152,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["import os\n","\n","if not os.getenv(\"IS_TESTING\"):\n","    # Automatically restart kernel after installs\n","    import IPython\n","\n","    app = IPython.Application.instance()\n","    app.kernel.do_shutdown(True)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"before_you_begin:nogpu"},"source":["## Before you begin\n","\n","### GPU runtime\n","\n","This tutorial does not require a GPU runtime.\n","\n","### Set up your Google Cloud project\n","\n","**The following steps are required, regardless of your notebook environment.**\n","\n","1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n","\n","2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n","\n","3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n","\n","4. [The Google Cloud SDK](https://cloud.google.com/sdk) is already installed in Google Cloud Notebook.\n","\n","5. Enter your project ID in the cell below. Then run the  cell to make sure the\n","Cloud SDK uses the right project for all the commands in this notebook.\n","\n","**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."]},{"cell_type":"code","metadata":{"id":"set_project_id","executionInfo":{"status":"ok","timestamp":1638509451174,"user_tz":480,"elapsed":184,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["PROJECT_ID = \"wav2vecemotionvoicerecognition\"  # @param {type:\"string\"}"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"autoset_project_id","executionInfo":{"status":"ok","timestamp":1638509452838,"user_tz":480,"elapsed":155,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n","    # Get your GCP project id from gcloud\n","    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n","    PROJECT_ID = shell_output[0]\n","    print(\"Project ID:\", PROJECT_ID)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"set_gcloud_project_id","executionInfo":{"status":"ok","timestamp":1638509455698,"user_tz":480,"elapsed":1381,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"eb76ab36-722a-41a2-b60d-16982d4090cd"},"source":["! gcloud config set project $PROJECT_ID"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n","\n","\n","To take a quick anonymous survey, run:\n","  $ gcloud survey\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"region"},"source":["#### Region\n","\n","You can also change the `REGION` variable, which is used for operations\n","throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n","\n","- Americas: `us-central1`\n","- Europe: `europe-west4`\n","- Asia Pacific: `asia-east1`\n","\n","You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n","\n","Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"]},{"cell_type":"code","metadata":{"id":"EGmLyGOC-VsH","executionInfo":{"status":"ok","timestamp":1638509457523,"user_tz":480,"elapsed":161,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["REGION = \"us-central1\"  # @param {type: \"string\"}"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"timestamp"},"source":["#### Timestamp\n","\n","If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."]},{"cell_type":"code","metadata":{"id":"gJHdQTrb-VsH","executionInfo":{"status":"ok","timestamp":1638509458544,"user_tz":480,"elapsed":165,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["from datetime import datetime\n","\n","TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcp_authenticate"},"source":["### Authenticate your Google Cloud account\n","\n","**If you are using Google Cloud Notebook**, your environment is already authenticated. Skip this step.\n","\n","**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n","\n","**Otherwise**, follow these steps:\n","\n","In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n","\n","**Click Create service account**.\n","\n","In the **Service account name** field, enter a name, and click **Create**.\n","\n","In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n","\n","Click Create. A JSON file that contains your key downloads to your local environment.\n","\n","Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."]},{"cell_type":"code","metadata":{"id":"UBQlyUt0-VsH","executionInfo":{"status":"ok","timestamp":1638509476703,"user_tz":480,"elapsed":16755,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["# If you are running this notebook in Colab, run this cell and follow the\n","# instructions to authenticate your GCP account. This provides access to your\n","# Cloud Storage bucket and lets you submit training jobs and prediction\n","# requests.\n","\n","import os\n","import sys\n","\n","# If on Google Cloud Notebook, then don't execute this code\n","if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n","    if \"google.colab\" in sys.modules:\n","        from google.colab import auth as google_auth\n","\n","        google_auth.authenticate_user()\n","\n","    # If you are running this notebook locally, replace the string below with the\n","    # path to your service account key and run this cell to authenticate your GCP\n","    # account.\n","    elif not os.getenv(\"IS_TESTING\"):\n","        %env GOOGLE_APPLICATION_CREDENTIALS ''"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bucket:mbsdk"},"source":["### Create a Cloud Storage bucket\n","\n","**The following steps are required, regardless of your notebook environment.**\n","\n","When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n","\n","Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."]},{"cell_type":"code","metadata":{"id":"bucket","executionInfo":{"status":"ok","timestamp":1638509484164,"user_tz":480,"elapsed":161,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["BUCKET_NAME = \"gs://audio_emotion_dataset\"  # @param {type:\"string\"}"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"autoset_bucket","executionInfo":{"status":"ok","timestamp":1638509485140,"user_tz":480,"elapsed":222,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n","    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"create_bucket"},"source":["**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJ6dfiV3-VsH","executionInfo":{"status":"ok","timestamp":1638509488199,"user_tz":480,"elapsed":1988,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"b1ff6ca2-1f9e-47f6-eb6a-c4006e16eda8"},"source":["! gsutil mb -l $REGION $BUCKET_NAME"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating gs://audio_emotion_dataset/...\n","ServiceException: 409 A Cloud Storage bucket named 'audio_emotion_dataset' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"]}]},{"cell_type":"markdown","metadata":{"id":"validate_bucket"},"source":["Finally, validate access to your Cloud Storage bucket by examining its contents:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucFqtspU-VsI","executionInfo":{"status":"ok","timestamp":1638509489351,"user_tz":480,"elapsed":1156,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"122fc516-83e5-43b6-bc5b-b06356d5125e"},"source":["! gsutil ls -al $BUCKET_NAME"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["      2134  2021-11-26T18:46:48Z  gs://audio_emotion_dataset/aiplatform-2021-11-26-18:46:48.678-aiplatform_custom_trainer_script-0.1.tar.gz#1637952408799864  metageneration=1\n","      1657  2021-11-27T21:06:26Z  gs://audio_emotion_dataset/aiplatform-2021-11-27-21:06:26.824-aiplatform_custom_trainer_script-0.1.tar.gz#1638047186996612  metageneration=1\n","      1662  2021-11-27T21:22:51Z  gs://audio_emotion_dataset/aiplatform-2021-11-27-21:22:51.266-aiplatform_custom_trainer_script-0.1.tar.gz#1638048171430347  metageneration=1\n","      1677  2021-11-27T21:30:17Z  gs://audio_emotion_dataset/aiplatform-2021-11-27-21:30:17.520-aiplatform_custom_trainer_script-0.1.tar.gz#1638048617724316  metageneration=1\n","      1666  2021-11-27T21:40:16Z  gs://audio_emotion_dataset/aiplatform-2021-11-27-21:40:16.641-aiplatform_custom_trainer_script-0.1.tar.gz#1638049216813954  metageneration=1\n","      1801  2021-11-27T22:04:34Z  gs://audio_emotion_dataset/aiplatform-2021-11-27-22:04:34.230-aiplatform_custom_trainer_script-0.1.tar.gz#1638050674404409  metageneration=1\n","      1802  2021-11-28T00:40:59Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-00:40:59.041-aiplatform_custom_trainer_script-0.1.tar.gz#1638060059139071  metageneration=1\n","      1813  2021-11-28T00:51:35Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-00:51:35.319-aiplatform_custom_trainer_script-0.1.tar.gz#1638060695410994  metageneration=1\n","      1808  2021-11-28T01:11:29Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-01:11:29.660-aiplatform_custom_trainer_script-0.1.tar.gz#1638061889760331  metageneration=1\n","      5508  2021-11-28T01:50:54Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-01:50:54.432-aiplatform_custom_trainer_script-0.1.tar.gz#1638064254537860  metageneration=1\n","      5515  2021-11-28T02:01:28Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-02:01:28.834-aiplatform_custom_trainer_script-0.1.tar.gz#1638064888948919  metageneration=1\n","      5534  2021-11-28T02:09:04Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-02:09:04.367-aiplatform_custom_trainer_script-0.1.tar.gz#1638065344478307  metageneration=1\n","      5529  2021-11-28T02:15:39Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-02:15:39.568-aiplatform_custom_trainer_script-0.1.tar.gz#1638065739697620  metageneration=1\n","      5530  2021-11-28T06:41:07Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-06:41:06.944-aiplatform_custom_trainer_script-0.1.tar.gz#1638081667106364  metageneration=1\n","      5578  2021-11-28T06:52:12Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-06:52:12.284-aiplatform_custom_trainer_script-0.1.tar.gz#1638082332477847  metageneration=1\n","      5573  2021-11-28T07:01:28Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-07:01:28.007-aiplatform_custom_trainer_script-0.1.tar.gz#1638082888146282  metageneration=1\n","      5572  2021-11-28T07:09:43Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-07:09:43.055-aiplatform_custom_trainer_script-0.1.tar.gz#1638083383166999  metageneration=1\n","      5565  2021-11-28T07:28:25Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-07:28:25.325-aiplatform_custom_trainer_script-0.1.tar.gz#1638084505534064  metageneration=1\n","      5573  2021-11-28T07:42:52Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-07:42:52.508-aiplatform_custom_trainer_script-0.1.tar.gz#1638085372611981  metageneration=1\n","      5569  2021-11-28T19:38:33Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-19:38:33.346-aiplatform_custom_trainer_script-0.1.tar.gz#1638128313450001  metageneration=1\n","      5573  2021-11-28T19:54:24Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-19:54:24.542-aiplatform_custom_trainer_script-0.1.tar.gz#1638129264643750  metageneration=1\n","      5573  2021-11-28T20:20:12Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-20:20:12.712-aiplatform_custom_trainer_script-0.1.tar.gz#1638130812888258  metageneration=1\n","      5581  2021-11-28T21:05:46Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-21:05:46.015-aiplatform_custom_trainer_script-0.1.tar.gz#1638133546161555  metageneration=1\n","      5647  2021-11-28T22:17:24Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-22:17:24.217-aiplatform_custom_trainer_script-0.1.tar.gz#1638137844326041  metageneration=1\n","      5648  2021-11-28T22:28:25Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-22:28:24.849-aiplatform_custom_trainer_script-0.1.tar.gz#1638138505050983  metageneration=1\n","      5659  2021-11-28T22:46:05Z  gs://audio_emotion_dataset/aiplatform-2021-11-28-22:46:05.473-aiplatform_custom_trainer_script-0.1.tar.gz#1638139565596165  metageneration=1\n","      5658  2021-11-29T15:15:35Z  gs://audio_emotion_dataset/aiplatform-2021-11-29-15:15:35.428-aiplatform_custom_trainer_script-0.1.tar.gz#1638198935623816  metageneration=1\n","      5665  2021-11-29T15:44:27Z  gs://audio_emotion_dataset/aiplatform-2021-11-29-15:44:27.223-aiplatform_custom_trainer_script-0.1.tar.gz#1638200667392238  metageneration=1\n","      5943  2021-11-29T16:15:01Z  gs://audio_emotion_dataset/aiplatform-2021-11-29-16:15:01.235-aiplatform_custom_trainer_script-0.1.tar.gz#1638202501476891  metageneration=1\n","      5667  2021-11-29T16:37:24Z  gs://audio_emotion_dataset/aiplatform-2021-11-29-16:37:24.029-aiplatform_custom_trainer_script-0.1.tar.gz#1638203844228955  metageneration=1\n","      5655  2021-11-29T17:19:00Z  gs://audio_emotion_dataset/aiplatform-2021-11-29-17:19:00.459-aiplatform_custom_trainer_script-0.1.tar.gz#1638206340623441  metageneration=1\n","      5653  2021-11-29T19:52:54Z  gs://audio_emotion_dataset/aiplatform-2021-11-29-19:52:54.174-aiplatform_custom_trainer_script-0.1.tar.gz#1638215574344581  metageneration=1\n","      5649  2021-11-30T23:33:29Z  gs://audio_emotion_dataset/aiplatform-2021-11-30-23:33:28.827-aiplatform_custom_trainer_script-0.1.tar.gz#1638315209010366  metageneration=1\n","      5647  2021-11-30T23:56:20Z  gs://audio_emotion_dataset/aiplatform-2021-11-30-23:56:20.636-aiplatform_custom_trainer_script-0.1.tar.gz#1638316580812466  metageneration=1\n","      5648  2021-12-01T00:12:57Z  gs://audio_emotion_dataset/aiplatform-2021-12-01-00:12:57.633-aiplatform_custom_trainer_script-0.1.tar.gz#1638317577797884  metageneration=1\n","      5660  2021-12-02T14:39:29Z  gs://audio_emotion_dataset/aiplatform-2021-12-02-14:39:29.437-aiplatform_custom_trainer_script-0.1.tar.gz#1638455969541873  metageneration=1\n","      5625  2021-12-02T14:49:49Z  gs://audio_emotion_dataset/aiplatform-2021-12-02-14:49:48.999-aiplatform_custom_trainer_script-0.1.tar.gz#1638456589114946  metageneration=1\n","      5377  2021-12-02T14:49:42Z  gs://audio_emotion_dataset/trainer_cifar10.tar.gz#1638456582209675  metageneration=1\n","                                 gs://audio_emotion_dataset/20211126184533/\n","                                 gs://audio_emotion_dataset/20211129151319/\n","                                 gs://audio_emotion_dataset/20211130233041/\n","                                 gs://audio_emotion_dataset/Dataset/\n","                                 gs://audio_emotion_dataset/Models/\n","TOTAL: 38 objects, 178594 bytes (174.41 KiB)\n"]}]},{"cell_type":"markdown","metadata":{"id":"setup_vars"},"source":["### Set up variables\n","\n","Next, set up some variables used throughout the tutorial.\n","### Import libraries and define constants"]},{"cell_type":"code","metadata":{"id":"import_aip:mbsdk","executionInfo":{"status":"ok","timestamp":1638509496795,"user_tz":480,"elapsed":1006,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["import google.cloud.aiplatform as aip"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"init_aip:mbsdk"},"source":["## Initialize Vertex SDK for Python\n","\n","Initialize the Vertex SDK for Python for your project and corresponding bucket."]},{"cell_type":"code","metadata":{"id":"e2J2Ax5q-VsI","executionInfo":{"status":"ok","timestamp":1638509497334,"user_tz":480,"elapsed":7,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"accelerators:training,cpu,prediction,cpu,mbsdk"},"source":["#### Set hardware accelerators\n","\n","You can set hardware accelerators for training and prediction.\n","\n","Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n","\n","    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n","\n","\n","Otherwise specify `(None, None)` to use a container image to run on a CPU.\n","\n","Learn more [here](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators) hardware accelerator support for your region\n","\n","*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3 -- which is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."]},{"cell_type":"code","metadata":{"id":"Q14-EKf3-VsI","executionInfo":{"status":"ok","timestamp":1638509499900,"user_tz":480,"elapsed":152,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n","    TRAIN_GPU, TRAIN_NGPU = (\n","        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n","        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n","    )\n","else:\n","    TRAIN_GPU, TRAIN_NGPU = (None, None)\n","\n","if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n","    DEPLOY_GPU, DEPLOY_NGPU = (\n","        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n","        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n","    )\n","else:\n","    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"container:training,prediction"},"source":["#### Set pre-built containers\n","\n","Set the pre-built Docker container image for training and prediction.\n","\n","\n","For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n","\n","\n","For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPJ8Lkrw-VsI","executionInfo":{"status":"ok","timestamp":1638509501728,"user_tz":480,"elapsed":150,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"9c9b001c-f673-4f09-b385-d49320a31689"},"source":["if os.getenv(\"IS_TESTING_TF\"):\n","    TF = os.getenv(\"IS_TESTING_TF\")\n","else:\n","    TF = \"2-1\"\n","\n","if TF[0] == \"2\":\n","    if TRAIN_GPU:\n","        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n","    else:\n","        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n","    if DEPLOY_GPU:\n","        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n","    else:\n","        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n","else:\n","    if TRAIN_GPU:\n","        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n","    else:\n","        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n","    if DEPLOY_GPU:\n","        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n","    else:\n","        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n","\n","TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/{}:latest\".format(TRAIN_VERSION)\n","DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/{}:latest\".format(DEPLOY_VERSION)\n","\n","print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n","print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training: gcr.io/cloud-aiplatform/training/tf-cpu.2-1:latest None None\n","Deployment: gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-1:latest None None\n"]}]},{"cell_type":"markdown","metadata":{"id":"machine:training,prediction"},"source":["#### Set machine type\n","\n","Next, set the machine type to use for training and prediction.\n","\n","- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n"," - `machine type`\n","     - `n1-standard`: 3.75GB of memory per vCPU.\n","     - `n1-highmem`: 6.5GB of memory per vCPU\n","     - `n1-highcpu`: 0.9 GB of memory per vCPU\n"," - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n","\n","*Note: The following is not supported for training:*\n","\n"," - `standard`: 2 vCPUs\n"," - `highcpu`: 2, 4 and 8 vCPUs\n","\n","*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prb4dCa2-VsI","executionInfo":{"status":"ok","timestamp":1638509503838,"user_tz":480,"elapsed":166,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"afd408fe-6e04-4f0b-fb2f-dcf54c9f712f"},"source":["if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n","    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n","else:\n","    MACHINE_TYPE = \"n1-highmem\"\n","\n","VCPU = \"8\"\n","TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n","print(\"Train machine type\", TRAIN_COMPUTE)\n","\n","if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n","    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n","else:\n","    MACHINE_TYPE = \"n1-highmem\"\n","\n","VCPU = \"8\"\n","DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n","print(\"Deploy machine type\", DEPLOY_COMPUTE)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Train machine type n1-highmem-8\n","Deploy machine type n1-highmem-8\n"]}]},{"cell_type":"markdown","metadata":{"id":"tutorial_start:custom"},"source":["# Tutorial\n","\n","Now you are ready to start creating your own custom model and training for CIFAR10."]},{"cell_type":"markdown","metadata":{"id":"examine_training_package"},"source":["### Examine the training package\n","\n","#### Package layout\n","\n","Before you start the training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n","\n","- PKG-INFO\n","- README.md\n","- setup.cfg\n","- setup.py\n","- trainer\n","  - \\_\\_init\\_\\_.py\n","  - task.py\n","\n","The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n","\n","The file `trainer/task.py` is the Python script for executing the custom training job. *Note*, when we referred to it in the worker pool specification, we replace the directory slash with a dot (`trainer.task`) and dropped the file suffix (`.py`).\n","\n","#### Package Assembly\n","\n","In the following cells, you will assemble the training package."]},{"cell_type":"code","metadata":{"id":"x3Lzj0mu-VsI","executionInfo":{"status":"ok","timestamp":1638509507784,"user_tz":480,"elapsed":1009,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["# Make folder for Python training script\n","! rm -rf custom\n","! mkdir custom\n","\n","# Add package information\n","! touch custom/README.md\n","\n","setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n","! echo \"$setup_cfg\" > custom/setup.cfg\n","\n","setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n   ],\\n\\n    packages=setuptools.find_packages())\"\n","! echo \"$setup_py\" > custom/setup.py\n","\n","pkg_info = \"Metadata-Version: 1.0\\n\\nName: CIFAR10 image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n","! echo \"$pkg_info\" > custom/PKG-INFO\n","\n","# Make the training subfolder\n","! mkdir custom/trainer\n","! touch custom/trainer/__init__.py"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"taskpy_contents:cifar10"},"source":["#### Task.py contents\n","\n","In the next cell, you write the contents of the training script task.py. We won't go into detail, it's just there for you to browse. In summary:\n","\n","- Get the directory where to save the model artifacts from the command line (`--model_dir`), and if not specified, then from the environment variable `AIP_MODEL_DIR`.\n","- Loads CIFAR10 dataset from TF Datasets (tfds).\n","- Builds a model using TF.Keras model API.\n","- Compiles the model (`compile()`).\n","- Sets a training distribution strategy according to the argument `args.distribute`.\n","- Trains the model (`fit()`) with epochs and steps according to the arguments `args.epochs` and `args.steps`\n","- Saves the trained model (`save(args.model_dir)`) to the specified model directory."]},{"cell_type":"code","metadata":{"id":"KzUQkGZG-VsJ","executionInfo":{"status":"ok","timestamp":1638509510913,"user_tz":480,"elapsed":153,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["# %%writefile custom/trainer/task.py\n","# # Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n","\n","# import tensorflow_datasets as tfds\n","# import tensorflow as tf\n","# from tensorflow.python.client import device_lib\n","# import argparse\n","# import os\n","# import sys\n","# tfds.disable_progress_bar()\n","\n","# parser = argparse.ArgumentParser()\n","# parser.add_argument('--model-dir', dest='model_dir',\n","#                     default=os.getenv(\"AIP_MODEL_DIR\"), type=str, help='Model dir.')\n","# parser.add_argument('--lr', dest='lr',\n","#                     default=0.01, type=float,\n","#                     help='Learning rate.')\n","# parser.add_argument('--epochs', dest='epochs',\n","#                     default=10, type=int,\n","#                     help='Number of epochs.')\n","# parser.add_argument('--steps', dest='steps',\n","#                     default=200, type=int,\n","#                     help='Number of steps per epoch.')\n","# parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n","#                     help='distributed training strategy')\n","# args = parser.parse_args()\n","\n","# print('Python Version = {}'.format(sys.version))\n","# print('TensorFlow Version = {}'.format(tf.__version__))\n","# print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n","# print('DEVICES', device_lib.list_local_devices())\n","\n","# # Single Machine, single compute device\n","# if args.distribute == 'single':\n","#     if tf.test.is_gpu_available():\n","#         strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n","#     else:\n","#         strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n","# # Single Machine, multiple compute device\n","# elif args.distribute == 'mirror':\n","#     strategy = tf.distribute.MirroredStrategy()\n","# # Multiple Machine, multiple compute device\n","# elif args.distribute == 'multi':\n","#     strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n","\n","# # Multi-worker configuration\n","# print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n","\n","# # Preparing dataset\n","# BUFFER_SIZE = 10000\n","# BATCH_SIZE = 64\n","\n","\n","# def make_datasets_unbatched():\n","\n","#   # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n","#   def scale(image, label):\n","#     image = tf.cast(image, tf.float32)\n","#     image /= 255.0\n","#     return image, label\n","\n","\n","#   datasets, info = tfds.load(name='cifar10',\n","#                             with_info=True,\n","#                             as_supervised=True)\n","#   return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n","\n","\n","# # Build the Keras model\n","# def build_and_compile_cnn_model():\n","#   model = tf.keras.Sequential([\n","#       tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n","#       tf.keras.layers.MaxPooling2D(),\n","#       tf.keras.layers.Conv2D(32, 3, activation='relu'),\n","#       tf.keras.layers.MaxPooling2D(),\n","#       tf.keras.layers.Flatten(),\n","#       tf.keras.layers.Dense(10, activation='softmax')\n","#   ])\n","#   model.compile(\n","#       loss=tf.keras.losses.sparse_categorical_crossentropy,\n","#       optimizer=tf.keras.optimizers.SGD(learning_rate=args.lr),\n","#       metrics=['accuracy'])\n","#   return model\n","\n","\n","# # Train the model\n","# NUM_WORKERS = strategy.num_replicas_in_sync\n","# # Here the batch size scales up by number of workers since\n","# # `tf.data.Dataset.batch` expects the global batch size.\n","# GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n","# train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n","\n","# with strategy.scope():\n","#   # Creation of dataset, and model building/compiling need to be within\n","#   # `strategy.scope()`.\n","#   model = build_and_compile_cnn_model()\n","\n","# model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps)\n","# model.save(args.model_dir)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ADJaQyuMCxS","executionInfo":{"status":"ok","timestamp":1638511681541,"user_tz":480,"elapsed":154,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"98091231-7433-49e5-ba4c-6d80daae8354"},"source":["%%writefile custom/trainer/task.py\n","# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","import argparse\n","import os\n","import sys\n","import torchaudio\n","\n","import gcsfs\n","fs = gcsfs.GCSFileSystem(project='wav2vecemotionvoicerecognition', token='cloud')\n","\n","tfds.disable_progress_bar()\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--model-dir', dest='model_dir',\n","                    default=os.getenv(\"AIP_MODEL_DIR\"), type=str, help='Model dir.')\n","parser.add_argument('--lr', dest='lr',\n","                    default=0.01, type=float,\n","                    help='Learning rate.')\n","parser.add_argument('--epochs', dest='epochs',\n","                    default=10, type=int,\n","                    help='Number of epochs.')\n","parser.add_argument('--steps', dest='steps',\n","                    default=200, type=int,\n","                    help='Number of steps per epoch.')\n","parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n","                    help='distributed training strategy')\n","args = parser.parse_args()\n","\n","print('Python Version = {}'.format(sys.version))\n","print('TensorFlow Version = {}'.format(tf.__version__))\n","print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n","print('DEVICES', device_lib.list_local_devices())\n","\n","# Single Machine, single compute device\n","if args.distribute == 'single':\n","    if tf.test.is_gpu_available():\n","        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n","    else:\n","        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n","# Single Machine, multiple compute device\n","elif args.distribute == 'mirror':\n","    strategy = tf.distribute.MirroredStrategy()\n","# Multiple Machine, multiple compute device\n","elif args.distribute == 'multi':\n","    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n","\n","# Multi-worker configuration\n","print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n","\n","def load_csv():\n","  # Loading the created dataset using datasets\n","  from datasets import load_dataset, load_metric\n","\n","  train_file = \"audio_emotion_dataset/Dataset/train.csv\"\n","  test_file = \"audio_emotion_dataset/Dataset/test.csv\"\n","\n","  data_files = {\n","      \"train\": train_file, \n","      \"validation\": test_file,\n","  }\n","  \n","  from datasets import Dataset\n","  import pandas as pd\n","  with fs.open(train_file, 'rb') as f:\n","\n","    df = pd.read_csv(f, sep='\\t') \n","    train_dataset = Dataset.from_pandas(df)\n","\n","  with fs.open(test_file, 'rb') as f:\n","\n","    df = pd.read_csv(f, sep='\\t') \n","    eval_dataset = Dataset.from_pandas(df)\n","\n","  return train_dataset, eval_dataset\n","\n","\n","train_dataset, eval_dataset = load_csv()\n","\n","# We need to specify the input and output column\n","input_column = \"path\"\n","output_column = \"emotion\"\n","\n","# we need to distinguish the unique labels in our SER dataset\n","label_list = train_dataset.unique(output_column)\n","label_list.sort()  # Let's sort it for determinism\n","num_labels = len(label_list)\n","# print(f\"A classification problem with {num_labels} classes: {label_list}\")\n","\n","from transformers import AutoConfig, Wav2Vec2Processor\n","\n","model_name_or_path = \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n","pooling_mode = \"mean\"\n","\n","processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n","target_sampling_rate = processor.feature_extractor.sampling_rate\n","print(f\"The target sampling rate: {target_sampling_rate}\")\n","\n","from transformers import Wav2Vec2ForCTC\n","\n","model = Wav2Vec2ForCTC.from_pretrained(\n","    \"facebook/wav2vec2-base\", \n","    ctc_loss_reduction=\"mean\", \n","    pad_token_id=processor.tokenizer.pad_token_id,\n",")\n","\n","# config\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=num_labels,\n","    label2id={label: i for i, label in enumerate(label_list)},\n","    id2label={i: label for i, label in enumerate(label_list)},\n","    finetuning_task=\"wav2vec2_clf\",\n",")\n","setattr(config, 'pooling_mode', pooling_mode)\n","\n","import gcsfs\n","fs = gcsfs.GCSFileSystem(project='wav2vecemotionvoicerecognition', token='cloud')\n","fs.get(\"audio_emotion_dataset\", './audio_emotion_dataset', recursive=True)\n","\n","def speech_file_to_array_fn(path):\n","    # import soundfile as sf\n","    # import io\n","\n","    # speech_array, sampling_rate = sf.read(file=io.BytesIO(f), dtype='float32')\n","    speech_array, sampling_rate = torchaudio.load(path)\n","    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n","    speech = resampler(speech_array).squeeze().numpy()\n","    return speech\n","\n","def label_to_id(label, label_list):\n","\n","    if len(label_list) > 0:\n","        return label_list.index(label) if label in label_list else -1\n","\n","    return label\n","\n","def preprocess_function(examples):\n","    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n","    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n","\n","    result = processor(speech_list, sampling_rate=target_sampling_rate)\n","    result[\"labels\"] = list(target_list)\n","\n","    return result\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batch_size=100,\n","    batched=True,\n","    num_proc=4\n",")\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batch_size=100,\n","    batched=True,\n","    num_proc=4\n",")\n","\n","idx = 0\n","print(f\"Training input_values: {train_dataset[idx]['input_values']}\")\n","print(f\"Training attention_mask: {train_dataset[idx]['attention_mask']}\")\n","print(f\"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['emotion']}\")\n","\n","from dataclasses import dataclass\n","from typing import Optional, Tuple\n","import torch\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SpeechClassifierOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model\n",")\n","\n","\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    \"\"\"Head for wav2vec classification task.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(config.final_dropout)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","\n","class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.pooling_mode = config.pooling_mode\n","        self.config = config\n","\n","        self.wav2vec2 = Wav2Vec2Model(config)\n","        self.classifier = Wav2Vec2ClassificationHead(config)\n","\n","        self.init_weights()\n","\n","    def freeze_feature_extractor(self):\n","        self.wav2vec2.feature_extractor._freeze_parameters()\n","\n","    def merged_strategy(\n","            self,\n","            hidden_states,\n","            mode=\"mean\"\n","    ):\n","        if mode == \"mean\":\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == \"sum\":\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == \"max\":\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            raise Exception(\n","                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n","\n","        return outputs\n","\n","    def forward(\n","            self,\n","            input_values,\n","            attention_mask=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","            labels=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        outputs = self.wav2vec2(\n","            input_values,\n","            attention_mask=attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        hidden_states = outputs[0]\n","        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n","        logits = self.classifier(hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SpeechClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Union\n","import torch\n","\n","import transformers\n","from transformers import Wav2Vec2Processor\n","\n","\n","@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","        max_length (:obj:`int`, `optional`):\n","            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n","        max_length_labels (:obj:`int`, `optional`):\n","            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n","        pad_to_multiple_of (:obj:`int`, `optional`):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","    max_length: Optional[int] = None\n","    max_length_labels: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    pad_to_multiple_of_labels: Optional[int] = None\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [feature[\"labels\"] for feature in features]\n","\n","        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n","\n","        return batch\n","\n","data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n","is_regression = False\n","\n","import numpy as np\n","from transformers import EvalPrediction\n","\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n","\n","    if is_regression:\n","        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n","    else:\n","        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n","\n","model = Wav2Vec2ForSpeechClassification.from_pretrained(\n","    model_name_or_path,\n","    config=config,\n",")\n","\n","model.freeze_feature_extractor()\n","\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    # output_dir=args.model_dir,\n","    output_dir=\"./\",\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=2,\n","    num_train_epochs=1.0,\n","    save_steps=1,\n","    eval_steps=1,\n","    logging_steps=1,\n","    learning_rate=1e-4,\n","    save_total_limit=2,\n",")\n","\n","from typing import Any, Dict, Union\n","\n","import torch\n","from packaging import version\n","from torch import nn\n","\n","from transformers import (\n","    Trainer,\n","    is_apex_available,\n",")\n","\n","if is_apex_available():\n","    from apex import amp\n","\n","if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n","    _is_native_amp_available = True\n","    from torch.cuda.amp import autocast\n","\n","\n","class CTCTrainer(Trainer):\n","    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n","        \"\"\"\n","        Perform a training step on a batch of inputs.\n","\n","        Subclass and override to inject custom behavior.\n","\n","        Args:\n","            model (:obj:`nn.Module`):\n","                The model to train.\n","            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n","                The inputs and targets of the model.\n","\n","                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n","                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n","\n","        Return:\n","            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n","        \"\"\"\n","\n","        model.train()\n","        inputs = self._prepare_inputs(inputs)\n","\n","        if self.use_amp:\n","            with autocast():\n","                loss = self.compute_loss(model, inputs)\n","        else:\n","            loss = self.compute_loss(model, inputs)\n","\n","        if self.args.gradient_accumulation_steps > 1:\n","            loss = loss / self.args.gradient_accumulation_steps\n","\n","        if self.use_amp:\n","            self.scaler.scale(loss).backward()\n","        elif self.use_apex:\n","            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","        elif self.deepspeed:\n","            self.deepspeed.backward(loss)\n","        else:\n","            loss.backward()\n","\n","        return loss.detach()\n","\n","trainer = CTCTrainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=processor.feature_extractor,\n",")\n","\n","# trainer.train()\n","trainer.save_model(\"./Models\")\n","processor.save_pretrained('./Models')\n","fs.put('./Models', \"audio_emotion_dataset/Models\", recursive=True)\n"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting custom/trainer/task.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"tarball_training_script"},"source":["#### Store training script on your Cloud Storage bucket\n","\n","Next, you package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ktW-wepW-VsJ","executionInfo":{"status":"ok","timestamp":1638511701801,"user_tz":480,"elapsed":1829,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"cbec405a-198b-4300-ffcd-b8bc07ba6080"},"source":["! rm -f custom.tar custom.tar.gz\n","! tar cvf custom.tar custom\n","! gzip custom.tar\n","! gsutil cp custom.tar.gz $BUCKET_NAME/trainer_cifar10.tar.gz"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["custom/\n","custom/PKG-INFO\n","custom/trainer/\n","custom/trainer/__init__.py\n","custom/trainer/task.py\n","custom/README.md\n","custom/setup.py\n","custom/setup.cfg\n","Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n","/ [1 files][  5.3 KiB/  5.3 KiB]                                                \n","Operation completed over 1 objects/5.3 KiB.                                      \n"]}]},{"cell_type":"markdown","metadata":{"id":"create_custom_training_job:mbsdk,no_model"},"source":["### Create and run custom training job\n","\n","\n","To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n","\n","#### Create custom training job\n","\n","A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n","\n","- `display_name`: The human readable name for the custom training job.\n","- `container_uri`: The training container image.\n","- `requirements`: Package requirements for the training container image (e.g., pandas).\n","- `script_path`: The relative path to the training script."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1RPZhqHM-VsJ","executionInfo":{"status":"ok","timestamp":1638511704179,"user_tz":480,"elapsed":158,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"cbd76bb4-b63a-423d-b2f2-8ad124a26e0b"},"source":["job = aip.CustomTrainingJob(\n","    display_name=\"cifar10_\" + TIMESTAMP,\n","    script_path=\"custom/trainer/task.py\",\n","    container_uri=TRAIN_IMAGE,\n","    requirements=[\"gcsfs==0.7.1\", \"tensorflow-datasets==4.4\", \"datasets\", \"transformers\", \"SoundFile\", \"tensorflow==2.7.0\", \"torch==1.10.0\", \"torchvision==0.11.1\", \"torchaudio==0.10.0\"],\n",")\n","\n","print(job)"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["<google.cloud.aiplatform.training_jobs.CustomTrainingJob object at 0x7f03709ac650>\n"]}]},{"cell_type":"markdown","metadata":{"id":"prepare_custom_cmdargs"},"source":["### Prepare your command-line arguments\n","\n","Now define the command-line arguments for your custom training container:\n","\n","- `args`: The command-line arguments to pass to the executable that is set as the entry point into the container.\n","  - `--model-dir` : For our demonstrations, we use this command-line argument to specify where to store the model artifacts.\n","      - direct: You pass the Cloud Storage location as a command line argument to your training script (set variable `DIRECT = True`), or\n","      - indirect: The service passes the Cloud Storage location as the environment variable `AIP_MODEL_DIR` to your training script (set variable `DIRECT = False`). In this case, you tell the service the model artifact location in the job specification.\n","  - `\"--epochs=\" + EPOCHS`: The number of epochs for training.\n","  - `\"--steps=\" + STEPS`: The number of steps per epoch."]},{"cell_type":"code","metadata":{"id":"vzxa7v86-VsJ","executionInfo":{"status":"ok","timestamp":1638511706478,"user_tz":480,"elapsed":166,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}}},"source":["# MODEL_DIR = \"{}/{}\".format(BUCKET_NAME, TIMESTAMP)\n","MODEL_DIR = \"{}/{}\".format(BUCKET_NAME, \"Models\")\n","EPOCHS = 20\n","STEPS = 100\n","\n","DIRECT = True\n","if DIRECT:\n","    CMDARGS = [\n","        \"--model-dir=\" + MODEL_DIR,\n","        \"--epochs=\" + str(EPOCHS),\n","        \"--steps=\" + str(STEPS),\n","    ]\n","else:\n","    CMDARGS = [\n","        \"--epochs=\" + str(EPOCHS),\n","        \"--steps=\" + str(STEPS),\n","    ]"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"run_custom_job:mbsdk,no_model"},"source":["#### Run the custom training job\n","\n","Next, you run the custom job to start the training job by invoking the method `run`, with the following parameters:\n","\n","- `args`: The command-line arguments to pass to the training script.\n","- `replica_count`: The number of compute instances for training (replica_count = 1 is single node training).\n","- `machine_type`: The machine type for the compute instances.\n","- `accelerator_type`: The hardware accelerator type.\n","- `accelerator_count`: The number of accelerators to attach to a worker replica.\n","- `base_output_dir`: The Cloud Storage location to write the model artifacts to.\n","- `sync`: Whether to block until completion of the job."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T46DPUoC-VsJ","executionInfo":{"status":"ok","timestamp":1638512285556,"user_tz":480,"elapsed":576561,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"}},"outputId":"dbf05e56-6c4f-4e84-81e2-04a56918af7a"},"source":["if TRAIN_GPU:\n","    job.run(\n","        args=CMDARGS,\n","        replica_count=1,\n","        machine_type=TRAIN_COMPUTE,\n","        accelerator_type=TRAIN_GPU.name,\n","        accelerator_count=TRAIN_NGPU,\n","        base_output_dir=MODEL_DIR,\n","        sync=True,\n","    )\n","else:\n","    job.run(\n","        args=CMDARGS,\n","        replica_count=1,\n","        machine_type=TRAIN_COMPUTE,\n","        base_output_dir=MODEL_DIR,\n","        sync=True,\n","    )\n","\n","model_path_to_deploy = MODEL_DIR"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n","gs://audio_emotion_dataset/aiplatform-2021-12-03-06:08:29.488-aiplatform_custom_trainer_script-0.1.tar.gz.\n","INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n","gs://audio_emotion_dataset/Models \n","INFO:google.cloud.aiplatform.training_jobs:View Training:\n","https://console.cloud.google.com/ai/platform/locations/us-central1/training/7999555122000560128?project=35996520627\n","INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n","https://console.cloud.google.com/ai/platform/locations/us-central1/training/2514205960235384832?project=35996520627\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob run completed. Resource name: projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128\n","WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/35996520627/locations/us-central1/trainingPipelines/7999555122000560128 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"]}]},{"cell_type":"markdown","metadata":{"id":"load_saved_model"},"source":["## Load the saved model\n","\n","Your model is stored in a TensorFlow SavedModel format in a Cloud Storage bucket. Now load it from the Cloud Storage bucket, and then you can do some things, like evaluate the model, and do a prediction.\n","\n","To load, you use the TF.Keras `model.load_model()` method passing it the Cloud Storage path where the model is saved -- specified by `MODEL_DIR`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Unjjv4VB-VsJ","executionInfo":{"elapsed":9814,"status":"ok","timestamp":1637952871246,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"826cddd1-bda2-493d-deed-c04f85f33d99"},"source":["import tensorflow as tf\n","\n","local_model = tf.keras.models.load_model(MODEL_DIR)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"]}]},{"cell_type":"markdown","metadata":{"id":"evaluate_custom_model:image"},"source":["## Evaluate the model\n","\n","Now find out how good the model is.\n","\n","### Load evaluation data\n","\n","You will load the CIFAR10 test (holdout) data from `tf.keras.datasets`, using the method `load_data()`. This returns the dataset as a tuple of two elements. The first element is the training data and the second is the test data. Each element is also a tuple of two elements: the image data, and the corresponding labels.\n","\n","You don't need the training data, and hence why we loaded it as `(_, _)`.\n","\n","Before you can run the data through evaluation, you need to preprocess it:\n","\n","`x_test`:\n","1. Normalize (rescale) the pixel data by dividing each pixel by 255. This replaces each single byte integer pixel with a 32-bit floating point number between 0 and 1.\n","\n","`y_test`:<br/>\n","2. The labels are currently scalar (sparse). If you look back at the `compile()` step in the `trainer/task.py` script, you will find that it was compiled for sparse labels. So we don't need to do anything more."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evaluate_custom_model:image,cifar10","executionInfo":{"elapsed":4908,"status":"ok","timestamp":1637952881073,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"c5b1ff3a-d89e-4c13-824b-7ff05e7ef45e"},"source":["import numpy as np\n","from tensorflow.keras.datasets import cifar10\n","\n","(_, _), (x_test, y_test) = cifar10.load_data()\n","x_test = (x_test / 255.0).astype(np.float32)\n","\n","print(x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n","170508288/170498071 [==============================] - 2s 0us/step\n","(10000, 32, 32, 3) (10000, 1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"perform_evaluation_custom"},"source":["### Perform the model evaluation\n","\n","Now evaluate how well the model in the custom job did."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTZqze3s-VsJ","executionInfo":{"elapsed":9206,"status":"ok","timestamp":1637952892027,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"7cf78cb2-758d-4010-d10c-6451f3cdf9d0"},"source":["local_model.evaluate(x_test, y_test)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 9s 3ms/step - loss: 1.7217 - accuracy: 0.1003\n"]},{"data":{"text/plain":["[1.7216861248016357, 0.10029999911785126]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"serving_function_image:xai"},"source":["### Serving function for image data\n","\n","To pass images to the prediction service, you encode the compressed (e.g., JPEG) image bytes into base 64 -- which makes the content safe from modification while transmitting binary data over the network. Since this deployed model expects input data as raw (uncompressed) bytes, you need to ensure that the base 64 encoded data gets converted back to raw bytes before it is passed as input to the deployed model.\n","\n","To resolve this, define a serving function (`serving_fn`) and attach it to the model as a preprocessing step. Add a `@tf.function` decorator so the serving function is fused to the underlying model (instead of upstream on a CPU).\n","\n","When you send a prediction or explanation request, the content of the request is base 64 decoded into a Tensorflow string (`tf.string`), which is passed to the serving function (`serving_fn`). The serving function preprocesses the `tf.string` into raw (uncompressed) numpy bytes (`preprocess_fn`) to match the input requirements of the model:\n","- `io.decode_jpeg`- Decompresses the JPG image which is returned as a Tensorflow tensor with three channels (RGB).\n","- `image.convert_image_dtype` - Changes integer pixel values to float 32.\n","- `image.resize` - Resizes the image to match the input shape for the model.\n","- `resized / 255.0` - Rescales (normalization) the pixel data between 0 and 1.\n","\n","At this point, the data can be passed to the model (`m_call`).\n","\n","#### XAI Signatures\n","\n","When the serving function is saved back with the underlying model (`tf.saved_model.save`), you specify the input layer of the serving function as the signature `serving_default`.\n","\n","For XAI image models, you need to save two additional signatures from the serving function:\n","\n","- `xai_preprocess`: The preprocessing function in the serving function.\n","- `xai_model`: The concrete function for calling the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1PuZHrN-VsK","executionInfo":{"elapsed":5678,"status":"ok","timestamp":1637952898259,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"0a0b6bd5-34af-456e-8ab0-7d7e6871635d"},"source":["CONCRETE_INPUT = \"numpy_inputs\"\n","\n","\n","def _preprocess(bytes_input):\n","    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n","    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n","    resized = tf.image.resize(decoded, size=(32, 32))\n","    rescale = tf.cast(resized / 255.0, tf.float32)\n","    return rescale\n","\n","\n","@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n","def preprocess_fn(bytes_inputs):\n","    decoded_images = tf.map_fn(\n","        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n","    )\n","    return {\n","        CONCRETE_INPUT: decoded_images\n","    }  # User needs to make sure the key matches model's input\n","\n","\n","@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n","def serving_fn(bytes_inputs):\n","    images = preprocess_fn(bytes_inputs)\n","    prob = m_call(**images)\n","    return prob\n","\n","\n","m_call = tf.function(local_model.call).get_concrete_function(\n","    [tf.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",")\n","\n","tf.saved_model.save(\n","    local_model,\n","    model_path_to_deploy,\n","    signatures={\n","        \"serving_default\": serving_fn,\n","        # Required for XAI\n","        \"xai_preprocess\": preprocess_fn,\n","        \"xai_model\": m_call,\n","    },\n",")"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","INFO:tensorflow:Assets written to: gs://audio_emotion_dataset/20211126184533/assets\n"]}]},{"cell_type":"markdown","metadata":{"id":"serving_function_signature:xai"},"source":["## Get the serving function signature\n","\n","You can get the signatures of your model's input and output layers by reloading the model into memory, and querying it for the signatures corresponding to each layer.\n","\n","When making a prediction request, you need to route the request to the serving function instead of the model, so you need to know the input layer name of the serving function -- which you will use later when you make a prediction request.\n","\n","You also need to know the name of the serving function's input and output layer for constructing the explanation metadata -- which is discussed subsequently."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0C3ZnaNu-VsK","executionInfo":{"elapsed":2692,"status":"ok","timestamp":1637952904375,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"217eef87-3d18-40eb-b8ff-f2a92b8fb4ac"},"source":["loaded = tf.saved_model.load(model_path_to_deploy)\n","\n","serving_input = list(\n","    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",")[0]\n","print(\"Serving function input:\", serving_input)\n","serving_output = list(loaded.signatures[\"serving_default\"].structured_outputs.keys())[0]\n","print(\"Serving function output:\", serving_output)\n","\n","input_name = local_model.input.name\n","print(\"Model input name:\", input_name)\n","output_name = local_model.output.name\n","print(\"Model output name:\", output_name)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Serving function input: bytes_inputs\n","Serving function output: output_0\n","Model input name: conv2d_input\n","Model output name: dense/Softmax:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"explanation_spec"},"source":["### Explanation Specification\n","\n","To get explanations when doing a prediction, you must enable the explanation capability and set corresponding settings when you upload your custom model to an Vertex `Model` resource. These settings are referred to as the explanation metadata, which consists of:\n","\n","- `parameters`: This is the specification for the explainability algorithm to use for explanations on your model. You can choose between:\n","  - Shapley - *Note*, not recommended for image data -- can be very long running\n","  - XRAI\n","  - Integrated Gradients\n","- `metadata`: This is the specification for how the algoithm is applied on your custom model.\n","\n","#### Explanation Parameters\n","\n","Let's first dive deeper into the settings for the explainability algorithm.\n","\n","#### Shapley\n","\n","Assigns credit for the outcome to each feature, and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values.\n","\n","Use Cases:\n","  - Classification and regression on tabular data.\n","\n","Parameters:\n","\n","- `path_count`: This is the number of paths over the features that will be processed by the algorithm. An exact approximation of the Shapley values requires M! paths, where M is the number of features. For the CIFAR10 dataset, this would be 784 (28*28).\n","\n","For any non-trival number of features, this is too compute expensive. You can reduce the number of paths over the features to M * `path_count`.\n","\n","#### Integrated Gradients\n","\n","A gradients-based method to efficiently compute feature attributions with the same axiomatic properties as the Shapley value.\n","\n","Use Cases:\n","  - Classification and regression on tabular data.\n","  - Classification on image data.\n","\n","Parameters:\n","\n","- `step_count`: This is the number of steps to approximate the remaining sum. The more steps, the more accurate the integral approximation. The general rule of thumb is 50 steps, but as you increase so does the compute time.\n","\n","#### XRAI\n","\n","Based on the integrated gradients method, XRAI assesses overlapping regions of the image to create a saliency map, which highlights relevant regions of the image rather than pixels.\n","\n","Use Cases:\n","\n","  - Classification on image data.\n","\n","Parameters:\n","\n","- `step_count`: This is the number of steps to approximate the remaining sum. The more steps, the more accurate the integral approximation. The general rule of thumb is 50 steps, but as you increase so does the compute time.\n","\n","In the next code cell, set the variable `XAI` to which explainabilty algorithm you will use on your custom model."]},{"cell_type":"code","metadata":{"id":"explanation_parameters:mbsdk"},"source":["XAI = \"ig\"  # [ shapley, ig, xrai ]\n","\n","if XAI == \"shapley\":\n","    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n","elif XAI == \"ig\":\n","    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n","elif XAI == \"xrai\":\n","    PARAMETERS = {\"xrai_attribution\": {\"step_count\": 50}}\n","\n","parameters = aip.explain.ExplanationParameters(PARAMETERS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"explanation_metadata:image"},"source":["#### Explanation Metadata\n","\n","Let's first dive deeper into the explanation metadata, which consists of:\n","\n","- `outputs`: A scalar value in the output to attribute -- what to explain. For example, in a probability output \\[0.1, 0.2, 0.7\\] for classification, one wants an explanation for 0.7. Consider the following formulae, where the output is `y` and that is what we want to explain.\n","\n","    y = f(x)\n","\n","Consider the following formulae, where the outputs are `y` and `z`. Since we can only do attribution for one scalar value, we have to pick whether we want to explain the output `y` or `z`. Assume in this example the model is object detection and y and z are the bounding box and the object classification. You would want to pick which of the two outputs to explain.\n","\n","    y, z = f(x)\n","\n","The dictionary format for `outputs` is:\n","\n","    { \"outputs\": { \"[your_display_name]\":\n","                   \"output_tensor_name\": [layer]\n","                 }\n","    }\n","\n","<blockquote>\n"," -  [your_display_name]: A human readable name you assign to the output to explain. A common example is \"probability\".<br/>\n"," -  \"output_tensor_name\": The key/value field to identify the output layer to explain. <br/>\n"," -  [layer]: The output layer to explain. In a single task model, like a tabular regressor, it is the last (topmost) layer in the model.\n","</blockquote>\n","\n","- `inputs`: The features for attribution -- how they contributed to the output. Consider the following formulae, where `a` and `b` are the features. We have to pick which features to explain how the contributed. Assume that this model is deployed for A/B testing, where `a` are the data_items for the prediction and `b` identifies whether the model instance is A or B. You would want to pick `a` (or some subset of) for the features, and not `b` since it does not contribute to the prediction.\n","\n","    y = f(a,b)\n","\n","The minimum dictionary format for `inputs` is:\n","\n","    { \"inputs\": { \"[your_display_name]\":\n","                  \"input_tensor_name\": [layer]\n","                 }\n","    }\n","\n","<blockquote>\n"," -  [your_display_name]: A human readable name you assign to the input to explain. A common example is \"features\".<br/>\n"," -  \"input_tensor_name\": The key/value field to identify the input layer for the feature attribution. <br/>\n"," -  [layer]: The input layer for feature attribution. In a single input tensor model, it is the first (bottom-most) layer in the model.\n","</blockquote>\n","\n","Since the inputs to the model are tabular, you can specify the following two additional fields as reporting/visualization aids:\n","\n","<blockquote>\n"," - \"modality\": \"image\": Indicates the field values are image data.\n","</blockquote>\n","\n","Since the inputs to the model are images, you can specify the following additional fields as reporting/visualization aids:\n","\n","<blockquote>\n"," - \"modality\": \"image\": Indicates the field values are image data.\n","</blockquote>"]},{"cell_type":"code","metadata":{"id":"explanation_metadata:mbsdk,image"},"source":["random_baseline = np.random.rand(32, 32, 3)\n","input_baselines = [{\"number_vaue\": x} for x in random_baseline]\n","\n","INPUT_METADATA = {\"input_tensor_name\": CONCRETE_INPUT, \"modality\": \"image\"}\n","\n","OUTPUT_METADATA = {\"output_tensor_name\": serving_output}\n","\n","input_metadata = aip.explain.ExplanationMetadata.InputMetadata(INPUT_METADATA)\n","output_metadata = aip.explain.ExplanationMetadata.OutputMetadata(OUTPUT_METADATA)\n","\n","metadata = aip.explain.ExplanationMetadata(\n","    inputs={\"image\": input_metadata}, outputs={\"class\": output_metadata}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"upload_model:mbsdk,xai"},"source":["## Upload the model\n","\n","Next, upload your model to a `Model` resource using `Model.upload()` method, with the following parameters:\n","\n","- `display_name`: The human readable name for the `Model` resource.\n","- `artifact`: The Cloud Storage location of the trained model artifacts.\n","- `serving_container_image_uri`: The serving container image.\n","- `sync`: Whether to execute the upload asynchronously or synchronously.\n","- `explanation_parameters`: Parameters to configure explaining for `Model`'s predictions.\n","- `explanation_metadata`: Metadata describing the `Model`'s input and output for explanation.\n","\n","If the `upload()` method is run asynchronously, you can subsequently block until completion with the `wait()` method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EU12_OuT-VsK","executionInfo":{"elapsed":276960,"status":"ok","timestamp":1637953199422,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"a6c9340c-32ff-4389-ac5d-41b1670127a7"},"source":["model = aip.Model.upload(\n","    display_name=\"cifar10_\" + TIMESTAMP,\n","    artifact_uri=MODEL_DIR,\n","    serving_container_image_uri=DEPLOY_IMAGE,\n","    explanation_parameters=parameters,\n","    explanation_metadata=metadata,\n","    sync=False,\n",")\n","\n","model.wait()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:google.cloud.aiplatform.models:Creating Model\n","INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/35996520627/locations/us-central1/models/6578989397448851456/operations/2715709126930857984\n","INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/35996520627/locations/us-central1/models/6578989397448851456\n","INFO:google.cloud.aiplatform.models:To use this Model in another session:\n","INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/35996520627/locations/us-central1/models/6578989397448851456')\n"]}]},{"cell_type":"markdown","metadata":{"id":"deploy_model:mbsdk,all"},"source":["## Deploy the model\n","\n","Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n","\n","- `deployed_model_display_name`: A human readable name for the deployed model.\n","- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n","If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n","If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n","- `machine_type`: The type of machine to use for training.\n","- `accelerator_type`: The hardware accelerator type.\n","- `accelerator_count`: The number of accelerators to attach to a worker replica.\n","- `starting_replica_count`: The number of compute instances to initially provision.\n","- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DHgVQn6-VsK","executionInfo":{"elapsed":503946,"status":"ok","timestamp":1637954084142,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"a29e47b3-f6cb-4ee6-afa2-2c72b4b208ed"},"source":["DEPLOYED_NAME = \"cifar10-\" + TIMESTAMP\n","\n","TRAFFIC_SPLIT = {\"0\": 100}\n","\n","MIN_NODES = 1\n","MAX_NODES = 1\n","\n","if DEPLOY_GPU:\n","    endpoint = model.deploy(\n","        deployed_model_display_name=DEPLOYED_NAME,\n","        traffic_split=TRAFFIC_SPLIT,\n","        machine_type=DEPLOY_COMPUTE,\n","        accelerator_type=DEPLOY_GPU,\n","        accelerator_count=DEPLOY_NGPU,\n","        min_replica_count=MIN_NODES,\n","        max_replica_count=MAX_NODES,\n","    )\n","else:\n","    endpoint = model.deploy(\n","        deployed_model_display_name=DEPLOYED_NAME,\n","        traffic_split=TRAFFIC_SPLIT,\n","        machine_type=DEPLOY_COMPUTE,\n","        accelerator_type=DEPLOY_GPU,\n","        accelerator_count=0,\n","        min_replica_count=MIN_NODES,\n","        max_replica_count=MAX_NODES,\n","    )"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:google.cloud.aiplatform.models:Creating Endpoint\n","INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/35996520627/locations/us-central1/endpoints/5687351438020182016/operations/6609071004792651776\n","INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/35996520627/locations/us-central1/endpoints/5687351438020182016\n","INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n","INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/35996520627/locations/us-central1/endpoints/5687351438020182016')\n","INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/35996520627/locations/us-central1/endpoints/5687351438020182016\n","INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/35996520627/locations/us-central1/endpoints/5687351438020182016/operations/7792391806884249600\n","INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/35996520627/locations/us-central1/endpoints/5687351438020182016\n"]}]},{"cell_type":"markdown","metadata":{"id":"get_test_item:test"},"source":["### Get test item\n","\n","You will use an example out of the test (holdout) portion of the dataset as a test item."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRtQA3Dp-VsK","executionInfo":{"elapsed":6,"status":"ok","timestamp":1637954084143,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"c21959d5-f8e2-4253-8423-27f6c6cb2325"},"source":["test_image = x_test[0]\n","test_label = y_test[0]\n","print(test_image.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 32, 3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"prepare_test_item:test,image"},"source":["### Prepare the request content\n","You are going to send the CIFAR10 image as compressed JPG image, instead of the raw uncompressed bytes:\n","\n","- `cv2.imwrite`: Use openCV to write the uncompressed image to disk as a compressed JPEG image.\n"," - Denormalize the image data from \\[0,1) range back to [0,255).\n"," - Convert the 32-bit floating point values to 8-bit unsigned integers.\n","- `tf.io.read_file`: Read the compressed JPG images back into memory as raw bytes.\n","- `base64.b64encode`: Encode the raw bytes into a base 64 encoded string."]},{"cell_type":"code","metadata":{"id":"aye8DIxC-VsK"},"source":["import base64\n","\n","import cv2\n","\n","cv2.imwrite(\"tmp.jpg\", (test_image * 255).astype(np.uint8))\n","\n","bytes = tf.io.read_file(\"tmp.jpg\")\n","b64str = base64.b64encode(bytes.numpy()).decode(\"utf-8\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"explain_request:mbsdk,custom,icn"},"source":["### Make the prediction with explanation\n","\n","Now that your `Model` resource is deployed to an `Endpoint` resource, one can do online explanations by sending prediction requests to the `Endpoint` resource.\n","\n","#### Request\n","\n","The format of each instance is:\n","\n","    [{serving_input: {'b64': bytes}]\n","\n","Since the explain() method can take multiple items (instances), send your single test item as a list of one test item.\n","\n","#### Response\n","\n","The response from the explain() call is a Python dictionary with the following entries:\n","\n","- `ids`: The internal assigned unique identifiers for each prediction request.\n","- `predictions`: The prediction per instance.\n","- `deployed_model_id`: The Vertex AI identifier for the deployed `Model` resource which did the predictions.\n","- `explanations`: The feature attributions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rX2lLiJt-VsK","executionInfo":{"elapsed":764,"status":"ok","timestamp":1637954085087,"user":{"displayName":"Wasae Qureshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisgOropTWDu9dJ8lUKqBAYcJUxPWmJoBOkqdfSGhU=s64","userId":"10636298441424021349"},"user_tz":480},"outputId":"755bf3db-f47e-487c-ee2b-b491e10f95a7"},"source":["instances_list = [{serving_input: {\"b64\": b64str}}]\n","\n","response = endpoint.explain(instances_list)\n","print(response)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction(predictions=[[0.0610445514, 0.139897987, 0.110223457, 0.101972289, 0.10750483, 0.0834006816, 0.13175644, 0.102561638, 0.0498607792, 0.111777402]], deployed_model_id='7938060536170676224', explanations=[attributions {\n","  baseline_output_value: 0.14060230553150177\n","  instance_output_value: 0.13989797234535217\n","  feature_attributions {\n","    struct_value {\n","      fields {\n","        key: \"image\"\n","        value {\n","          struct_value {\n","            fields {\n","              key: \"b64_jpeg\"\n","              value {\n","                string_value: \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDxppotS0+Frvy4BF5i70+QbtqbSRgluRghRwMdBinvpRvYYWeIWDwxojo0fMgGcvxzuzjggdV55FO1G1Gl6XJCiq5W/wByyEK3yhTtDZ6/xcYxwexGZ4YLzUEtpfMS2KIEE0SZbBAPA4K4+QYH98kfKa89ztHmg7K7/q3meW52jzwdo3dv+G87/wCRT1+3jzHcQlMp+6mAGDu+8GOeTuBPPP3fvGsStXWI54UtkuZ0edt7uijhSSB1zj+HbgYA2dO5yq6sP/DWtzswulJa3Ot1aBr2CaOEM++4V1KKX4O75uOduDnIz29ayr8Jptt9lSUNM5+dI5WKxFQUY8HBL88EZA4xzVjUNYj/ALZhmt7l3tFO5ljDL1LBuuPm2k88YyBnjNYUr+ZM75c7mJy7bm59T3PvWGHpTsubbf8A4c58LRnyxUtt7efn9ws88lzO80pBkc5YhQuT64FR0UV2JJKyO9JJWR//2Q==\"\n","              }\n","            }\n","          }\n","        }\n","      }\n","    }\n","  }\n","  output_index: 1\n","  approximation_error: 0.00730778282586079\n","  output_name: \"class\"\n","}\n","])\n"]}]},{"cell_type":"markdown","metadata":{"id":"understanding_explanations:cifar10"},"source":["### Understanding the explanations response\n","\n","Preview the images and their predicted classes without the explanations. Why did the model predict these classes?"]},{"cell_type":"code","metadata":{"id":"aM4gIUBb-VsL"},"source":["from io import BytesIO\n","\n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","\n","CLASSES = [\n","    \"airplane\",\n","    \"automobile\",\n","    \"bird\",\n","    \"cat\",\n","    \"deer\",\n","    \"dog\",\n","    \"frog\",\n","    \"horse\",\n","    \"ship\",\n","    \"truck\",\n","]\n","\n","# Note: change the `ig_response` variable below if you didn't deploy an IG model\n","for prediction in response.predictions:\n","    label_index = np.argmax(prediction)\n","    class_name = CLASSES[label_index]\n","    confidence_score = prediction[label_index]\n","    print(\n","        \"Predicted class: \"\n","        + class_name\n","        + \"\\n\"\n","        + \"Confidence score: \"\n","        + str(confidence_score)\n","    )\n","\n","    image = base64.b64decode(b64str)\n","    image = BytesIO(image)\n","    img = mpimg.imread(image, format=\"JPG\")\n","\n","    plt.imshow(img, interpolation=\"nearest\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"visualize_image_explanations"},"source":["### Visualize the images with AI Explanations\n","\n","The images returned show the explanations for only the top class predicted by the model. This means that if one of the model's predictions is incorrect, the pixels you see highlighted are for the *incorrect class*. For example, if the model predicted \"airplane\" when it should have predicted \"cat\", you can see explanations for why the model classified this image as an airplane.\n","\n","If you deployed an Integrated Gradients model, you can visualize its feature attributions. Currently, the highlighted pixels returned from AI Explanations show the top 60% of pixels that contributed to the model's prediction. The pixels you see after running the cell below show the pixels that most signaled the model's prediction."]},{"cell_type":"code","metadata":{"id":"LDytbazK-VsL"},"source":["import io\n","\n","for explanation in response.explanations:\n","    attributions = dict(explanation.attributions[0].feature_attributions)\n","    label_index = explanation.attributions[0].output_index[0]\n","    class_name = CLASSES[label_index]\n","    b64str = attributions[\"image\"][\"b64_jpeg\"]\n","    image = base64.b64decode(b64str)\n","    image = io.BytesIO(image)\n","    img = mpimg.imread(image, format=\"JPG\")\n","\n","    plt.imshow(img, interpolation=\"nearest\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"undeploy_model:mbsdk"},"source":["## Undeploy the model\n","\n","When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."]},{"cell_type":"code","metadata":{"id":"oGWXVN7k-VsL"},"source":["endpoint.undeploy_all()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cleanup:mbsdk"},"source":["# Cleaning up\n","\n","To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n","project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n","\n","Otherwise, you can delete the individual resources you created in this tutorial:\n","\n","- Dataset\n","- Pipeline\n","- Model\n","- Endpoint\n","- AutoML Training Job\n","- Batch Job\n","- Custom Job\n","- Hyperparameter Tuning Job\n","- Cloud Storage Bucket"]},{"cell_type":"code","metadata":{"id":"eXryVPPN-VsL"},"source":["delete_all = True\n","\n","if delete_all:\n","    # Delete the dataset using the Vertex dataset object\n","    try:\n","        if \"dataset\" in globals():\n","            dataset.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    # Delete the model using the Vertex model object\n","    try:\n","        if \"model\" in globals():\n","            model.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    # Delete the endpoint using the Vertex endpoint object\n","    try:\n","        if \"endpoint\" in globals():\n","            endpoint.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    # Delete the AutoML or Pipeline trainig job\n","    try:\n","        if \"dag\" in globals():\n","            dag.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    # Delete the custom trainig job\n","    try:\n","        if \"job\" in globals():\n","            job.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    # Delete the batch prediction job using the Vertex batch prediction object\n","    try:\n","        if \"batch_predict_job\" in globals():\n","            batch_predict_job.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n","    try:\n","        if \"hpt_job\" in globals():\n","            hpt_job.delete()\n","    except Exception as e:\n","        print(e)\n","\n","    if \"BUCKET_NAME\" in globals():\n","        ! gsutil rm -r $BUCKET_NAME"],"execution_count":null,"outputs":[]}]}